---
title: 'Part 1: Getting Started'
author: "Anthony Pulvino"
---
  
  
# **Setup instructions**
  
## Welcome to my tutorial on the basics of scraping the web using R!
  
This is an *R Notebook* that has code chunks inline with explanations. Run the code blocks by pressing the *Run* button while the cursor is in that code chunk, or by the key combination *Cmd+Shift+Enter*. Each code chunk behaves kind of like an isolated `.R` script file, but the results appear beneath the code chunk, instead of in the console.

```{r RUN THIS}
#install.packages("rvest")
library(rvest) # Web scraping
#install.packages("tidyverse")
library(tidyverse) # Data wrangling
#install.packages("RCurl")
library(RCurl) # Downloading files from the internet
#install.packages("stringr")
library(stringr)

demo_mode <- TRUE # Skip the the code that takes more than a few minutes to run

```

If there are no errors, then you are ready to go onto the next part. Otherwise, remove the hashes before the install.packages() functions and run those lines as well.

# Reading web source code

We are going to use a tool built-in to your web browser called Inspector view.
This feature allows us to view the underlying HTML code responsible for visible aspects of the websites you view.

For more information on HTML and other languages for building web-based applications visit:
https://github.com/nuitrcs/IntroToHTMLCSSJS

After opening a new window in a modern web browser such as Firefox, Chrome, Safari, or Edge, open some page from Wikipedia, such as https://en.wikipedia.org/wiki/Purple_Rain_(film).

## Using Inspector view
(Tested on Chrome, Safari, Edge and Firefox)

Right-click any element (such as a picture or title) on the web page you want to 'inspect'  and click 'Inspect'/'Inspect Element'. This will open a new window that shows the HTML code (and more) corresponding to the element you selected.

Take a minute with the built-in selector tool (usually a cross hairs or magnifying class icon) to explore different elements of the page. Some of the most common HTML tags you will see are div, span, a, table, img, p, ul, and li. We will use these tags to extract relevant information from the page.

Using this tool, you can browse the code and see how it was interpreted into the visual layout that you see, you can see the CSS style properties for every element, and you can even test changes to any of the website's code in real time.

*Web scraping partly depends on the uniformity of the underlying code*

Web code is extremely variable in quality and style, and so the first step to extracting data from a webpage is always understanding how the data is displayed.

**Quick exercise!**
Let's practice grabbing some of the html code associated with elements in the Wikipedia.org homepage.\
Highlight and right click an element on the page, Click 'Inspect'\
Right Click the highlighted portion of the html code in the Inspector View pane.\
Hover over "Copy", then select 'Copy selector' from the dropdown.\

Type in the chat the selector code for an element of the page and write which element it is!

Here's the code picture of the day (02/20/2025) (e.g., `#mp-tfa-img > div > span > a > img`)

## HTML versus CSS versus JS

Most websites use two major file types, HTML and CSS along with JS.
HTML (hyper text markup language) is a data file that contains code representing text, links to pictures, tables of data, and everything else.
CSS (cascading style sheets) contains code that browsers use to visually style the HTML.
JS (JavaScript) is a language used to create interactive elements on dynamic webpages that may change according to location or time, for example.

We can scrape elements of websites using their HTML (hierarchically grouped) OR CSS (stylistically grouped).
Advanced users might get use out of xpath as well, but that is beyond the scope of this workshop.

## Principles of scraping
Because the code for websites is often poorly written (sorry, webdevs), I want to offer some guidelines to help decide when and how to develop a web scraping solution for your project.

Rule 1. Don't scrape what you can download/access via API
Rule 2. Don't scrape what you cannot easily clean
Rule 3. Convert data into native data types ASAP (from strings)
Application Programming Interface (API) refers to software which allows the communication of data or other functionalities.
APIs can make scraping inefficient and in some cases scraping attempts may actively be blocked on websites that maintain an API.

# Scraping Wikipedia
*For our first example, we will scrape some lists from Wikipedia.*

Let's compare a list of films set in Minnesota (A) to a list of films actually shot in Minnesota (B). I want to use these lists to answer the simple question, do films shot in Minnesota tend to be set in Minnesota?

Link A. https://en.wikipedia.org/wiki/Category:Films_set_in_Minnesota
Link B. https://en.wikipedia.org/wiki/Category:Films_shot_in_Minnesota

Movie titles in these lists are represented in bullet pointed lists organized into alphabetical categories. The code for the first category ("0-9", in h3 tags) looks like this:

```
  <div class="mw-category-group"><h3>0â€“9</h3>
  <ul><li><a href="/wiki/20/20:_In_an_Instant" title="20/20: In an Instant">20/20: In an Instant</a></li>
  <li><a href="/wiki/360_(film)" title="360 (film)">360 (film)</a></li></ul></div>
```

## Common HTML Tags

Here is a quick breakdown of these tags:
`div` means 'division', which is used to apply the "mw-category-group" CSS class to this chunk of HTML
`ul` means 'unorderd list' = bullet point list
`li` means 'list item' = individual bullet point
`a` means 'anchor link' = normal hyperlink
`class` is for CSS styling, which we refer to with `#` and `.` like `#mw-category-group`

**Quick exercise!**
While you have the Selector tool open, click somewhere in the pane and CTRL+F.\
Have a search for one of the tags above... Tell us in the chat which one you searched and what content from the page is inside of it?\
Are there many elements of the page associated with that overarching tag? How many?

We only need the names of the films, but if we wanted to know more about these films later (say, their release date or budget), we might want the link to their Wikipedia page.

Check out one of the movie's pages to get a sense for the data potential: https://en.wikipedia.org/wiki/Purple_Rain_(film)

*With this in mind, let's scrape films set and shot in Minnesota to get the titles of the films (the text in the <a> tag) and their links (the <a> tag's 'href').*

For other curious people, here's a quick link to the trailer for the Purple Rain film: https://www.youtube.com/watch?v=AuXK8ZbTmLk It won an Oscar.


## Scraping the movie titles and links

```{r}
# Download the html of the two links into R variables
films_set_html <- read_html("https://en.wikipedia.org/wiki/Category:Films_set_in_Minnesota")

films_shot_html <- read_html("https://en.wikipedia.org/wiki/Category:Films_shot_in_Minnesota")

# to get the html object/link converted to a text-based string object we used html_text from rvest
html_text(films_shot_html)
```

**Quick exercise!**
When running html_text above, the output is not very comprehensible.\
We can use the function word() (from the stringr package) to subset certain portions of the text\
Use the word function to explore making subsets of text from the original output we got from running html_text above

```{r}
# function: word()
# input: html_text(films_shot_html)

#<your code here!>

```

**Another Quick exercise!**
You can see the results of our subset are still a bit tricky to parse... 
Is there another R function we could use to get a list of the movies in a more readable format?
```{r}

#<your code here!>

```


## Use web developer tools
Next we will scrape the data we want by using the information from Inspector View to write some inclusion/exclusion rules. Usually you can simply right-click an example of the information you want in Inspector View and use that to build your rule. *When you right-click the line in Inspector View, hover over 'Copy' and click 'Copy Selector'.* On the page Films Set in Minnesota this gives us:

```
#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a
```

If we paste that into the `html_nodes()` function

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a')

```

**Quick exercise!**
You can see that the above code returns some useful information about one of the films in the list of those set in Minnesota... but if we were doing an analysis and wanted to save just the title for some kind of analysis of movies across states...
What other rvest function would we need to pipe to?
Hint: we've already used it once before...

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a') %>%
  # <add function call here>

```


But we want every film, not just the first one. If you look at the rule, you will see two references to `nth-child(1)`, which is a way to specify CSS styles based on parent-child relationships and order (with ':').

Delete it to include all of the films in all of the categories:

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_text() # this extracts text from within HTML tags

```

But what do we mean by parent child relationships and order here...?
Take a moment to adjust the numerical options in the original line of code that included div:nth-child

What does div:nth-child() actually specify? li:nth-child()?
```{r}
# play with numeric values to nth-child spec here:
films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a') %>%
  html_text()

```



Now let's finish the job by storing the links and titles in R variables.

We can start by grabbing all of the movie titles that we were looking at earlier, now we'll store them as a variable "films_set_titles"
```{r}
# Titles, same as above
films_set_titles <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 
films_set_titles
```
Next we'll use the html_attr() function to get the portion of the webpage links 
```{r}

# The rule works equally well for the other link, too
films_set_links <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")
films_set_links
```

```{r}
# Join the titles and links as a data frame
films_set_mn <- data.frame("title" = films_set_titles, "link" = films_set_links)
films_set_mn
```

Now we can have a peek at the first couple of films set in minnesota
```{r}
# Peek
head(films_set_mn)
```

Let's remove some of the variables here that we no longer need.
```{r}
# Cleanup
rm(films_set_html, films_set_titles, films_set_links)
```

**Quick exercise!**
We follow the same process to generate the films_shot list:
Use the short code blocks from above as a template, and write the same set of variables for the "films_shot_html" variable we initialized earlier.

```{r}

# Titles, same as above
films_shot_titles <- films_shot_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 

# The rule works equally well for the other link, too
films_shot_links <- films_shot_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")

# Join the titles and links as a data frame
films_shot_mn <- data.frame("title" = films_shot_titles, "link" = films_shot_links)

# Peek
head(films_shot_mn)

# Cleanup
rm(films_shot_html, films_shot_titles, films_shot_links)
```


## Do films shot in Minnesota tend to be set in Minnesota?

```{r}

# Films shot in MN, set in MN
# intersect will find the films that are both shot and set in Minnesota
length(intersect(films_shot_mn$title,
                 films_set_mn$title)) / length(films_shot_mn)
# Films shot in MN, NOT set in MN
# setdiff will find all the films shot in Minnesota that AREN'T set in Minnesota... it turns out there are more of them
length(setdiff(films_shot_mn$title,
               films_set_mn$title)) / length(films_shot_mn)

```

In the context of films shot in Minnesota, most are NOT set there.



# How to scrape multiple pages

Let's get a little closer to Northwestern and get the titles and links for films set and shot in Chicago.

*But there is a problem.* There are many more films set and shot in Chicago than in Minnesota, and Wikipedia only lists 200 items per list per page. See for yourself:

```{r}
# same as before

films_set_chicago <- read_html("https://en.wikipedia.org/wiki/Category:Films_set_in_Chicago") %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_text()

length(films_set_chicago)
```

The list only has *200* items in it, but according to the link we are scraping, the full list is about twice that size. If we were browsing Wikipedia, we could click "next page" and see how the list continues, but that's not how we're reading it.

This is something you should note about websites you're scraping--how do you get all of the data to be represented in the HTML? 'Next' button? Infinite scroll?

## Crawling

One solution to this issue is to scrape every page and remove duplicates we find. But, this will take a lot of time and it will burden our post-processing. Instead, we will use the recursive property of pagination (that the next page can have a next page, which can have a next page) to crawl all pages, one at a time.

```{r}
###### We can write a couple of convenience functions to make our job of scraping a bit simpler for this project
###### The first function will take full web URL ("full_url") and the html encoding for an attribute of interest ("rule")

## The first two lines write a variable containing user input URLs, the base URL is wikipedia, 
## but the extension to the specific page after that is input by the user
## the next line sets a to.continue variable, this variable stores information about whether there's a second page
## the if statement evaluates the content of the variable, if there is a next page/TRUE, we store the successive URLs
## for the successive pages storing all the links as we go

# return the urls of the next pages
w.tree <- function(rel_url){
  
  root = "https://en.wikipedia.org"
  
  full_url <- paste0(root, rel_url)

  # see if there's a next page link
  to.continue <- read_html( full_url ) %>% 
             html_node("#mw-pages > a:last-child") %>% # the 'next page' link is the last link in this div
             html_text() %>%
             all.equal("next page")
  
  # if so, get the link
  if(to.continue == TRUE){
    
    next.page <- read_html( full_url ) %>% 
      html_node("#mw-pages > a:last-child") %>% # the 'next page' link is the last link in this div
      html_attr('href')

    w.tree(next.page) # recurse
    
    return(next.page)
  }
}
```

```{r}

## This function takes a full_url and a selector string rule as input and returns a dataframe containing two columns
## The first column is the_titles of movies on the wikipedia page
## the second column is the_links to the wikipedia pages for each of those movies
w.scrape <- function(full_url, rule){

  # get titles
  the_titles <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_text()
  
  # get links
  the_links <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_attr('href')
  
  # as a dataframe
  df <- data.frame("titles" = the_titles, "links" = the_links,
                   stringsAsFactors = FALSE) 
  
  return ( df)
}
```


```{r}
# convenience function to make a list of urls and their text from wikipedia category pages
#### Here the function takes the relative URL (after the wikipedia, "root"), 
#### and gets all successive page links (you can see the call to w.tree on the second line)
#### the next line writes an empty dataframe 
#### in the for loop that follows, we iterate over the URLs and store within the dataframe
#### the listed information on the page (our case will be the movie titles), 
#### and the relative links associated with each movie's wiki page

w.list <- function(rel_url, rule){
  
  root = "https://en.wikipedia.org"

  to.scrape <- c(rel_url, w.tree( rel_url ) ) # not tested beyond 2 pages
  
  output <- data.frame() # container
  
  for(page in to.scrape){
    output <- rbind( w.scrape( paste0(root, page), rule ), output )
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
  }
  
  return(unique(output)) # return unique rows
  
}

```

When we connect to a web page we increase traffic, and if we're doing heavy duty scraping this traffic can burden the host site.
We're using loops so that we can pause between iterations using Sys.sleep()... this allows us to scrape in a more friendly way, giving the host server more time between our attempts to access webpages of interest.


Here, we can use the functions we wrote above. We put them to use in this block of code by grabbing all of the films set in chicago.\
We start by defining the rule to get the movie titles (child_rule)\
Then we make our call to the function w.list() passing two arguments,\
The relative URL, or the unique portion associated with the specific movie wikipedia page\
and the html encoding for those movie titles.
```{r message=FALSE, warning=FALSE}

child_rule = "#mw-pages li a"

# let's see how it works
films_set_chicago <- w.list(rel_url = "/wiki/Category:Films_set_in_Chicago",
                            rule = child_rule)
```

When we call our films_set_chicago variable we get a very nice dataframe of the relevant information
```{r}
films_set_chicago

```

I get 400 films, which means we have just under 2 pages of items.
That should be all of them!

**Quick exercise!**
Let's zoom out and get global... Pick a country (home country or country of interest). \
Update the rel_url argument according to which country you selected... \
Tell us in the chat which country you picked and how many movies are set there!
```{r}

# Give it a try!
w.list(rel_url = #<your rel_url here!>,
       rule = child_rule)


```


## Scale it up

We could scrape every city or every state, or both, using the same basic methods as we employed for the Chicago list and the URLS. Doing so means touching many more HTML pages, increasing the chances we will hit an error.

Flow control refers to a mechanism that regulates the communication of data between two computers. This concept is important for handling exceptions (managing how a program responds to certain conditions).
We can use `next` and `break` to handle exceptions where we want the loop to skip the current item, or stop altogether.

*The code chunk below will scrape all of the state-level pages of films on Wikipedia.*

```{r message=FALSE, warning=FALSE}

# let's get all of the movies from these links
rel_link_list <- c( "/wiki/Category:Films_set_in_the_United_States_by_state",
                    "/wiki/Category:Films_shot_in_the_United_States_by_state")

# using the Inspector tool on 'Films set in Akron, Ohio' I copied the selector path
# I also had to delete the 'child' selectors, as I did before
#parent_rule <- '#mw-subcategories li a'
parent_rule <- '#mw-subcategories > div > div > div > ul > li > div > div.CategoryTreeItem > a'
child_rule = "#mw-pages li a"

# loop all the geographical area links to get all the list page links
# scrape all the geo categories
for(link in rel_link_list){
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  geo_links <- w.list( rel_url = link, rule = parent_rule )
  
  geo_content <- data.frame(matrix(ncol = 5, nrow = 0))
  
  # for each geo category scrape the links
  for(i in 1:nrow(geo_links)) {
    
    temp <- w.list( rel_url = paste0( geo_links$links[i] ) ,
                  rule = child_rule )
    
    if(nrow(temp) == 0){ 
      
      next # if our rule fails, skip this link
      
    } else {
      
          temp$parent_title <- geo_links$titles[i]
          temp$parent_link <- geo_links$links[i]
    
          geo_content <- rbind(geo_content, temp)
      }

    rm(temp)
    
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
    
  }#inner
}#outer

#rm(link, i, geo_links, rel_link_list) # cleanup

# saving this so you can load it without running it
# saveRDS(geo_content, "geo_content.rds") # 6062 films set in various states

```

This just shows how you can easily scale-up some simple scraping script into something with more of an appetite.

In the next part we will practice some web scraping, then we will build on this in one more part.



