---
title: 'Part 1: Getting Started'
author: "Anthony Pulvino"
---

  
# **Setup instructions**

## Welcome to my tutorial on the basics of scraping the web using R!
  
This is an *R Notebook* that has code chunks inline with explanations.
Run the code blocks by pressing the *Run* button while the cursor is in that code chunk, or by the key combination *Cmd+Shift+Enter*. Each code chunk behaves kind of like an isolated `.R` script file, but the results appear beneath the code chunk, instead of in the console.

```{r RUN THIS}
#install.packages("rvest")
library(rvest) # Web scraping
#install.packages("tidyverse")
library(tidyverse) # Data wrangling
#install.packages("RCurl")
library(RCurl) # Downloading files from the internet
#install.packages("stringr")
library(stringr)

demo_mode <- TRUE # Skip the the code that takes more than a few minutes to run

```

If there are no errors, then you are ready to go onto the next part. Otherwise, remove the hashes before the install.packages() functions and run those lines as well.

# Why web scraping?

There is a universe of information publicly available and a multitude of different academic disciplines that stand to benefit from collecting (or scraping) portions of this information to assist with different research projects and add valuable context to on-going projects.

As a biology PhD student, much of what I know about web scraping comes from my work as a consultant for RCDS. Others need my help scraping information, and I learn how to collect this information in a very curated way that suits the 'as needed' basis of their situation.

If you have little experience with web scraping, this is a great opportunity to learn in a slow way. We'll introduce *some* foundation concepts and apply them to scraping wikipedia pages. This is likely not an application that will directly apply to your research, but what you will be left with is some practical context for getting started and a few .Rmd files that can help inspire the assembly of your own scraping scripts which target the websites and repositories important for your work.

# Reading web source code

We are going to use a tool built-in to your web browser called Inspector view.
This feature allows us to view the underlying HTML code responsible for visible aspects of the websites you view.
For more information on HTML and other languages for building web-based applications visit:
https://github.com/nuitrcs/IntroToHTMLCSSJS

After opening a new window in a modern web browser such as Firefox, Chrome, Safari, or Edge, navigate to:
https://en.wikipedia.org/

## Using Inspector view
(Tested on Chrome, Safari, Edge and Firefox)

Right-click any element (such as a picture or title) on the web page you want to 'inspect'  and click 'Inspect'/'Inspect Element'.
This will open a new window that shows the HTML code/or "source code", corresponding to the element you selected.

Take a minute with the built-in selector tool to explore different elements of the page.
As you drag your cursor down the Selector Tool pane, you'll notice different portions of the webpage become highlighted.

You'll notice a few common "phrases" like div, span, a, table, img, p, ul, and li. 
These are referred to as HTML tags, and will reference these in the code we work with today to extract info from the web content we look at

*Web scraping partly depends on the uniformity of the underlying code*

Web code is extremely variable in quality and style, and in the languages its written in.
We can rely on a few key principles of scraping to make sure that we're controlling for this variability as thoughtfully as possible!

## Principles of scraping
When developing a web scraping solution for your own projects it's important to keep the following in mind:

Rule 1. Avoid scraping what you can download/access via API
**Note:** Application Programming Interface (API) refers to software which allows the communication of data or other functionalities.\
APIs can make scraping inefficient and in some cases scraping attempts may actively be blocked on websites that maintain an API.

Rule 2. Avoid scraping what you cannot easily "clean"
Rule 3. Convert data into "native" data types ASAP


**Quick exercise!**
Let's practice grabbing some of the html code associated with elements from the Wikipedia homepage we opened a bit ago.
Highlight and right click an element on the page, Click 'Inspect'
Right Click the highlighted portion of the html code in the Inspector View pane.
Hover over "Copy", then select 'Copy selector' from the dropdown.

Type in the chat the selector code for an element of the page and write which element it is!

Here's the code picture of the day (02/20/2025) (e.g., `#mp-tfa-img > div > span > a > img`)


# Scraping Film Wiki Pages
*For our first example, we will scrape some lists from Wikipedia.*

Let's compare a list of films set in Oregon (A) to a list of films actually shot in Oregon (B). 

I want to use these lists to answer the simple question, do films shot in Oregon tend to be set in Oregon?

Link A. https://en.wikipedia.org/wiki/Category:Films_set_in_Oregon
Link B. https://en.wikipedia.org/wiki/Category:Films_shot_in_Oregon

Movie titles in these lists are represented in bullet pointed lists organized into alphabetical categories.

Can you use the Selector Tool to copy and paste the code responsible for the entire block of 0-9 category of information in the chat?

```
<div class="mw-category-group"><h3>0â€“9</h3>
<ul><li><a href="/wiki/8_Seconds" title="8 Seconds">8 Seconds</a></li>
<li><a href="/wiki/10_Days_in_a_Madhouse" title="10 Days in a Madhouse">10 Days in a Madhouse</a></li>
<li><a href="/wiki/1941_(film)" title="1941 (film)">1941 (film)</a></li></ul></div>

```
Which of the HTML tags mentioned earlier is responsible for containing the entire block of "0-9" category movies for the Films shot in Oregon page? Can you type it in the chat?


**Quick exercise!**
While you have the Selector tool open, click somewhere in the pane and CTRL+F.\
Have a search for one of the tags we mentioned earlier:
div, span, a, table, img, p, ul, and li

Tell us in the chat which one you searched and what content from the page is inside of it? If you want to go back to the Wiki homepage we started on, there may be more to explore there!

Are there many elements of the page associated with that "overarching" tag? How many?...


## Now let's return to the movie example we were working on

We only need the names of the films, but if we wanted to know more about these films later (say, their release date or budget), we might want the link to each movie's specific Wikipedia pages.

Let's go to the films shot in Oregon page and have a look at the potential information we could grab:
https://en.wikipedia.org/wiki/Animal_House

*With this in mind, let's scrape films set and shot in Oregon to get the titles of the films (the text in the <a> tag) and their links (the <a> tag's 'href').*



## Scraping the movie titles and links

```{r}
# Download the html of the two links into R variables
films_set_html <- read_html("https://en.wikipedia.org/wiki/Category:Films_set_in_Oregon")

films_shot_html <- read_html("https://en.wikipedia.org/wiki/Category:Films_shot_in_Oregon")

# to get the html object/link converted to a text-based string object we used html_text from rvest
html_text(films_shot_html)
```

**Quick exercise!**
When running html_text() above, the output is not very comprehensible.\
We can use the function word() (from the stringr package) to subset certain portions of the text

Use the word function to explore making subsets of text from the original output we got from running html_text above

```{r}
# function: word()
# input: html_text(films_shot_html)

html_text(films_shot_html)

```



**Another Quick exercise!**
You can see the results of our subset are still a bit tricky to parse... 
Is there another R function we could use to get a list of the movies in a more readable format?
```{r}

html_text(films_shot_html)

```


## Use web developer tools
Next we will scrape the data we want by using the information from Inspector View.

Usually you can simply right-click an example of the information you want in Inspector View and use that.

*When you right-click the line in Inspector View, hover over 'Copy' and click 'Copy Selector'.* 

If you do this on any film in the bulleted lists, you get a string that should look similar to  this:

```
'#mw-pages > div > div > div:nth-child(2) > ul > li:nth-child(3) > a'

```

If we paste that into the `html_nodes()` function, we can run the code and see we're able to get a copy of the HTML source code we took from the Inspector View.

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(2) > ul > li:nth-child(3) > a')

```

**Quick exercise!**
You can see that the above code returns some useful information about one of the films in the list of those set in Oregon... but if we were doing an analysis and wanted to save just the title for some kind of analysis of movies across states...

What other rvest function would we need to pipe to?
Hint: we've already used it once before...

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(2) > ul > li:nth-child(3) > a') %>%
  #<put function here>


```

**Quick exercise!**
Now what if we want every film... not just the first one.

If you look at your rule/html code, you will see references to `nth-child()`, which is a way to specify "parent-child" relationships in CSS styled code.

But what does that actually mean? What happens if we remove all ":nth-child()" from our Selector String?

```{r}

films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(2) > ul > li:nth-child(3) > a') %>%
  html_text() # this extracts text from within HTML tags

```

Let's take a moment to consider this "parent-child" thing more closely before we move on.

What does div:nth-child() actually specify? li:nth-child()? 

Hint: Take a second to adjust the numeric values below. What do you notice about what changes in your output?...

```{r}
# play with numeric values to nth-child spec here:
films_set_html %>%
  html_nodes('#mw-pages > div > div > div:nth-child(1) > ul > li:nth-child(1) > a') %>%
  html_text()

```


Now let's scrape the links and titles from the Movies Set in Oregon wiki page and store them as variables.

We can start by grabbing movie titles we were looking at earlier, and we'll keep those in the "films_set_titles" variable

```{r}
# Titles, same as above
films_set_titles <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 
head(films_set_titles)
```

Next we'll use the html_attr() function to get the links to each movie's specific webpage. Since the wikipedia.org prefix is the same we'll forego collecting those here

```{r}

# The rule works equally well for the other link, too
films_set_links <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")
head(films_set_links)
```

Now we'll put this information together as a dataframe with two columns for each variable we initiated

```{r}
# Join the titles and links as a data frame
films_set_or <- data.frame("title" = films_set_titles, "link" = films_set_links)
head(films_set_or)

```

Now that we have our dataframe to work from, let's remove these individual variables from our environment

```{r}
# Cleanup
rm(films_set_html, films_set_titles, films_set_links)
```

**Quick exercise!**
We can follow the same process above to generate the films_shot list:
Use the short code blocks from above as a template (written below for you), and write update variables for the films *shot* in Oregon that we initialized earlier.

```{r}

# Titles, same as above
films_set_titles <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>% 
  html_text() 

# The rule works equally well for the other link, too
films_set_links <- films_set_html %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_attr("href")

# Join the titles and links as a data frame
films_set_or <- data.frame("title" = films_set_titles, "link" = films_set_links)

#  have a Peek
head(films_set_or)

# Cleanup since we have our complete dataframe now
rm(films_set_html, films_set_titles, films_set_links)
```


## Do films shot in Oregon tend to be set in Oregon?

```{r}

# Films shot in OR, set in OR
# intersect will find the films that are both shot and set in Oregon
length(intersect(films_shot_or$title,
                 films_set_or$title)) / length(films_shot_or)
# Films shot in OR, NOT set in OR
# setdiff will find all the films shot in Oregon that AREN'T set in Oregon... it turns out there are more of them
length(setdiff(films_shot_or$title,
               films_set_or$title)) / length(films_shot_or)

```

In the context of films shot in Oregon, most are NOT set there.



# How to scrape multiple pages

Let's get a little closer to Northwestern and get the titles and links for films set and shot in Chicago.

*But there is a problem.* There are many more films set and shot in Chicago than in Oregon, and Wikipedia only lists 200 items per list per page. See for yourself:

```{r}

# same as before
films_set_chicago <- read_html("https://en.wikipedia.org/wiki/Category:Films_set_in_Chicago") %>%
  html_nodes('#mw-pages > div > div > div > ul > li > a') %>%
  html_text()

length(films_set_chicago)

```

The list only has *200* items in it, but according to the link we are scraping, we have multiple pages... See "(next page)" at the bottom

You may come across this frequently in your own web scraping journeys and so it's valuable to carefully consider whether you're collecting all of the information you actually intend to avoid accidental subsets.


From here, things are going to increase in complexity... if we're starting to run out of time, I will try to move quickly through this.

Follow along as best you can, and feel free to ask questions a



## Putting it all together (pagination)!

We will use the recursive property of pagination (that the next page can have a next page, which can have a next page) to scrape each page, one at a time.

We can write a couple of convenience functions to make this easier. The first function will take full web URL ("full_url") and the html code for the webpage attribute we want to scrape ("rule").

```{r}

## We start by writing our function into a variable wiki tree ("w.tree").
## The function takes relative URLs!... Like the ones we kept in the dataframes we were working with above

# Inside the function we start by setting a variable containing our URLs... the wikipedia.org main URL, PASTED with the input variable which has the specific extension for each movie.

## Next we read the html and check if there is a "next page" button at the bottom of the page.
## we store this information in the "to.continue" variable

## Next we recursively scrape the information on the successive pages, IF there is a "next page" button at the bottom

# return the urls of the next pages
w.tree <- function(rel_url){
  
  ## set variable containing FULL, input URL
  full_url <- paste0("https://en.wikipedia.org", rel_url)

  # see if there's a next page link; read_html() the full link to the wiki page, index where there would be a "next page" button with html_node(), convert that to the actual "next page" text with html_text() and use all.equal() to evaluate if what we scraped was "next page"
  to.continue <- read_html( full_url ) %>% 
             html_node("#mw-pages > a:last-child") %>%
             html_text() %>%
             all.equal("next page")
  
  # if what we scraped was "next page", saved to the to.continue variable above
  if(to.continue == TRUE){
    
    # we scrape alllll of the movie titles on the next page
    next.page <- read_html( full_url ) %>% 
      html_node("#mw-pages > a:last-child") %>%
      html_attr('href')
    
    # and then we do it again until there's no more "next page"
    w.tree(next.page) # recurse
    
    # at the end, we return all of the movies we scraped across all pages
    return(next.page)
  }
}
```

Lets write another function to help us write scrape the information we want with the variables we previously wrote (for getting titles/relative links)!

```{r}

## This function takes a full_url and a selector string rule as input and returns a dataframe containing the information as two columns

w.scrape <- function(full_url, rule){

  ## get movie titles on wiki pages
  the_titles <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_text()
  
  # get links to wiki movie pages
  the_links <- read_html(full_url) %>%
    html_nodes( rule ) %>% 
    html_attr('href')
  
  # generate dataframe!
  ## The first column is the_titles of movies on the wikipedia page
  ## the second column is the_links to the wikipedia pages for each of those movies
  df <- data.frame("titles" = the_titles, "links" = the_links,
                   stringsAsFactors = FALSE) 
  
  return ( df)
}
```

Now let's collect all the relative links and get all the associated pages and write our final dataframe like we did previously

```{r}

w.list <- function(rel_url, rule){
 
   ## We start by saving the relative links to scrape,
  ## and we have a call to our old w.tree() function! 
  ## This is so we get information across all pages!
  to.scrape <- c(rel_url, w.tree( rel_url ) ) # not tested beyond 2 pages
  
  # let's set an empty dataframe to fill up
  output <- data.frame() # container
  
  ## Now we iterate over the URLs and store within the dataframe all of our movie titles! 
  ## as well as the links associated with each movie's wiki page
  for(page in to.scrape){
    output <- rbind( w.scrape( paste0("https://en.wikipedia.org", page), rule ), output )
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
  }
  
  return(unique(output)) # return unique rows
  
}

```

When we connect to a web page we increase traffic, and if we're doing heavy duty scraping this traffic can burden the host site.
We're using loops so that we can pause between iterations using Sys.sleep()... this allows us to scrape in a more friendly way, giving the host server more time between our attempts to access webpages of interest.


Here, we can use the functions we wrote above. We put them to use in this block of code by grabbing all of the films set in chicago.\
We start by defining the rule to get the movie titles (child_rule)\
Then we make our call to the function w.list() passing two arguments,\
The relative URL, or the unique portion associated with the specific movie wikipedia page\
and the html encoding for those movie titles.

```{r message=FALSE, warning=FALSE}

child_rule = "#mw-pages li a"

# let's see how it works
films_set_chicago <- w.list(rel_url = "/wiki/Category:Films_set_in_Chicago",
                            rule = child_rule)
```

When we call our films_set_chicago variable we get a very nice dataframe of the relevant information
```{r}

head(films_set_chicago)

```

I get 400 films, which means we have just under 2 pages of items.
That should be all of them!

**Quick exercise!**
Let's zoom out and get global... Pick a country (home country or country of interest). \
Update the rel_url argument according to which country you selected... \
Tell us in the chat which country you picked and how many movies are set there!
```{r}

# Give it a try!
w.list(rel_url = #<your rel_url here!>,
       rule = child_rule)


```














Are you hitting an hour by the time you get here? If so, delete this for tomorrow, and we can work it into part two or something else for next iteration of this workshop!



## Scale it up

We could scrape every city or every state, or both, using the same basic methods as we employed for the Chicago list and the URLS. Doing so means touching many more HTML pages, increasing the chances we will hit an error.

Flow control refers to a mechanism that regulates the communication of data between two computers. This concept is important for handling exceptions (managing how a program responds to certain conditions).
We can use `next` and `break` to handle exceptions where we want the loop to skip the current item, or stop altogether.

*The code chunk below will scrape all of the state-level pages of films on Wikipedia.*

```{r message=FALSE, warning=FALSE}

# let's get all of the movies from these links
rel_link_list <- c( "/wiki/Category:Films_set_in_the_United_States_by_state",
                    "/wiki/Category:Films_shot_in_the_United_States_by_state")

# using the Inspector tool on 'Films set in Akron, Ohio' I copied the selector path
# I also had to delete the 'child' selectors, as I did before
#parent_rule <- '#mw-subcategories li a'
parent_rule <- '#mw-subcategories > div > div > div > ul > li > div > div.CategoryTreeItem > a'
child_rule = "#mw-pages li a"

# loop all the geographical area links to get all the list page links
# scrape all the geo categories
for(link in rel_link_list){
  
  if(demo_mode == TRUE){ break }  # this takes a while
  
  geo_links <- w.list( rel_url = link, rule = parent_rule )
  
  geo_content <- data.frame(matrix(ncol = 5, nrow = 0))
  
  # for each geo category scrape the links
  for(i in 1:nrow(geo_links)) {
    
    temp <- w.list( rel_url = paste0( geo_links$links[i] ) ,
                  rule = child_rule )
    
    if(nrow(temp) == 0){ 
      
      next # if our rule fails, skip this link
      
    } else {
      
          temp$parent_title <- geo_links$titles[i]
          temp$parent_link <- geo_links$links[i]
    
          geo_content <- rbind(geo_content, temp)
      }

    rm(temp)
    
    Sys.sleep(0.5) # pause 1/2 second before scraping the next page
    
  }#inner
}#outer

#rm(link, i, geo_links, rel_link_list) # cleanup

# saving this so you can load it without running it
# saveRDS(geo_content, "geo_content.rds") # 6062 films set in various states

```

This just shows how you can easily scale-up some simple scraping script into something with more of an appetite.

In the next part we will practice some web scraping, then we will build on this in one more part.


